{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d78d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints video data to the screen\n",
    "def display_data(y_playlist_data):\n",
    "\n",
    "    for video in y_playlist_data:\n",
    "        print(f'''Published Date: {video['snippet']['publishedAt']}\n",
    "        \\n\\nTitle: {video['snippet']['title']}\n",
    "        \\n\\nDescription: {video['snippet']['description']}\n",
    "        \\n\\nVideo Id: {video['contentDetails']['videoId']}\n",
    "        \\n\\nThumbnails: \n",
    "        \\nDefault: {video['snippet']['thumbnails']['default']['url']},\n",
    "        \\nMax Res: {video['snippet']['thumbnails']['maxres']['url']},\n",
    "        \\nStandard: {video['snippet']['thumbnails']['standard']['url']}\\n\\n''')\n",
    "\n",
    "# helper function to create a file for the video descriptions        \n",
    "def path_to_data_path(path):\n",
    "    return path.replace('.csv','_Descriptions.csv')\n",
    "\n",
    "# Saves the video data to a new file. If the file exists the data is appended.\n",
    "def save_data(y_playlist_data,save_path,file_exists=False):\n",
    "    # check of the file is being saved as a csv\n",
    "    if not save_path.endswith('.csv'):\n",
    "        print('File must be saved as a .csv')\n",
    "        return []\n",
    "    \n",
    "    # Create the path for the video descriptions\n",
    "    save_desc_path = path_to_data_path(save_path)\n",
    "    \n",
    "    # Create lists to store the metadata\n",
    "    published_dates = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    video_ids = []\n",
    "    default_thumbnails = []\n",
    "    max_res_thumbnails = []\n",
    "    standard_thumbnails = []\n",
    "    \n",
    "    \n",
    "    for video in y_playlist_data:\n",
    "        # Store the metadata in the lists\n",
    "        published_dates.append(video['snippet']['publishedAt'])\n",
    "        titles.append(video['snippet']['title'])\n",
    "        descriptions.append(video['snippet']['description'])\n",
    "        video_ids.append(video['contentDetails']['videoId'])\n",
    "        \n",
    "        # use try blocks to extract potentially missing json data\n",
    "        try:\n",
    "            default_thumbnails.append(video['snippet']['thumbnails']['default']['url'])\n",
    "        except Excaption as e:\n",
    "            default_thumbnails.append(None)\n",
    "        try: \n",
    "            max_res_thumbnails.append(video['snippet']['thumbnails']['maxres']['url'])\n",
    "        except Exception as e:\n",
    "            max_res_thumbnails.append(None)\n",
    "        try:\n",
    "            standard_thumbnails.append(video['snippet']['thumbnails']['standard']['url'])\n",
    "        except Exception as e:\n",
    "            standard_thumbnails.append(None)\n",
    "    \n",
    "    # Merge the data to form the final dataset\n",
    "    final_data = {'Published_Date':published_dates, 'Video_Title': titles, 'Video_ID':video_ids, 'Thumbnail_Default': default_thumbnails, 'Thumbnail_Standard':standard_thumbnails, 'Thumbnail_Max_Res': max_res_thumbnails} \n",
    "    final_desc = {'Video_ID':video_ids, 'Description':descriptions}\n",
    "    \n",
    "    # Create dataframes for the video and description data\n",
    "    playlist_data = pd.DataFrame(final_data)\n",
    "    playlist_desc = pd.DataFrame(final_desc)\n",
    "    \n",
    "    # Save the files\n",
    "    if file_exists:\n",
    "        existing_file = pd.read_csv(save_path)\n",
    "        new_file = pd.concat([existing_file,playlist_data])\n",
    "        new_file.to_csv(save_path, encoding='utf-8', index=False)\n",
    "        \n",
    "        existing_desc_file = pd.read_csv(save_desc_path)\n",
    "        new_desc_file = pd.concat([existing_desc_file,playlist_desc])\n",
    "        new_desc_file.to_csv(save_desc_path, encoding='utf-8', index=False)\n",
    "        \n",
    "    else:\n",
    "        # Save into csv format in the desired location\n",
    "        playlist_data.to_csv(save_path, encoding='utf-8', index=False)\n",
    "        playlist_desc.to_csv(save_desc_path, encoding='utf-8', index=False)\n",
    "    \n",
    "    # return the video dataframe\n",
    "    return playlist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your api key\n",
    "api_key = 'Your API Key Goes Here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the youtube client to make api calls\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "\n",
    "# To gather all (up to the last 20,000) uploaded videos\n",
    "# we must have the ID of the Uploads playlist on a YouTube channel.\n",
    "# The Uploads playlist ID is the same as the channel ID but with the\n",
    "# second letter changed from a C to a U.\n",
    "\n",
    "# Fox News Channel id: UCXIJgqnII2ZOINSWNOGFThA\n",
    "# Fox News Uploads id: UUXIJgqnII2ZOINSWNOGFThA\n",
    "\n",
    "# CNN Channel id: UCupvZG-5ko_eiXAupbDfxWw\n",
    "# CNN Uploads id: UUupvZG-5ko_eiXAupbDfxWw\n",
    "\n",
    "# PBS Channel id: UC6ZFN9Tx6xh-skXCuRHCDpQ\n",
    "# PBS Uploads id: UU6ZFN9Tx6xh-skXCuRHCDpQ\n",
    "\n",
    "# NBC News Channel id: UCeY0bbntWzzVIaj2z3QigXg\n",
    "# NBC News Uploads id: UUeY0bbntWzzVIaj2z3QigXg\n",
    "\n",
    "# BlazeTV Channel id: UCKgJEs_v0JB-6jWb8lIy9Xw\n",
    "# BlazeTV Uploads id: UUKgJEs_v0JB-6jWb8lIy9Xw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa942813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the news channel to locate the channel id\n",
    "request = youtube.search().list(q='BlazeTV',part='snippet',type='channel',maxResults=5)\n",
    "response= request.execute()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/how-to-extract-youtube-video-titles-using-the-youtube-data-api-45d3f4998486\n",
    "\n",
    "# Uses the uploads playlist id to get all uploaded videos (capped at 20,000)\n",
    "def youtube_playlist_data(playlist_id, cut_off=20000, results_per_page=50, token=None):\n",
    "    # Results are capped at 50 per request via YouTube\n",
    "    if results_per_page > 50:\n",
    "        print('You can only request 50 results per page max. Please lower the number of results per page.')\n",
    "        return []\n",
    "    \n",
    "    video_data = []\n",
    "    \n",
    "    # A variable responsible for breaking out of the loop if the cutoff point is reached\n",
    "    query_counter = 0\n",
    "    \n",
    "    # The while loop continues until the items are present in the playlist\n",
    "    while True:\n",
    "        # Create the request\n",
    "        r = youtube.playlistItems().list(playlistId=playlist_id,\n",
    "                                         part='snippet, contentDetails',\n",
    "                                         maxResults=results_per_page,\n",
    "                                         pageToken=token)\n",
    "        \n",
    "        # Execute the request\n",
    "        y_playlist_data = r.execute()\n",
    "\n",
    "        # Increment the number of records recovered\n",
    "        query_counter += results_per_page\n",
    "        \n",
    "        #Store the data\n",
    "        video_data += y_playlist_data['items']\n",
    "\n",
    "        # Update the token so it can be used to get the next page of data\n",
    "        token = y_playlist_data.get('nextPageToken')\n",
    "\n",
    "        # If there is no token or we've reached a cutoff point break the loop\n",
    "        if (token is None) or (query_counter >= cut_off):\n",
    "            break\n",
    "        \n",
    "    # Return the final collected data and the token\n",
    "    return video_data, token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "data, last_token= youtube_playlist_data('UUKgJEs_v0JB-6jWb8lIy9Xw', results_per_page=50, token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If last_token is none, we've reached the end of the videos.\n",
    "# If there is a value, it can be passed to the youtube_playlist_data \n",
    "# function to set a starting point for more data collection.\n",
    "last_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65587519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file name prefix\n",
    "prefix = 'BlazeTV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the file names\n",
    "file_path = f'/Data/{prefix}_Data.csv'\n",
    "data_file_path = path_to_data_path(file_path)\n",
    "print(file_path,'\\n\\n',data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec026ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the files\n",
    "df = save_data(data,file_path,file_exists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the data looks good\n",
    "df_data = pd.read_csv(file_path)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fae4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the data looks good\n",
    "df_desc = pd.read_csv(data_file_path)\n",
    "df_desc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
